# 05_mcpClient3.py
# FastMCPã‚’æ´»ç”¨ã—ãŸã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆæŠ½è±¡åŒ–ç‰ˆ
# ãƒ„ãƒ¼ãƒ«ç®¡ç†ã€ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆæŠ½è±¡åŒ–ã€ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°ã‚’çµ±åˆ

import asyncio
import json
import re
from typing import Dict, Any, List, Optional, Tuple
from fastmcp import Client
from openai import OpenAI
from dotenv import load_dotenv
import os
from dataclasses import dataclass
from enum import Enum

# ç’°å¢ƒå¤‰æ•°ã‚’èª­ã¿è¾¼ã¿
load_dotenv()

# è¨­å®šã‚¯ãƒ©ã‚¹
@dataclass
class LLMConfig:
    model: str = "gpt-4o-mini"
    temperature: float = 0.1
    max_tokens: int = 500
    api_key: str = ""

@dataclass
class AgentConfig:
    max_iterations: int = 5
    debug_mode: bool = True
    fallback_to_direct: bool = True
    timeout: int = 30

class ToolDecision(Enum):
    TOOL = "TOOL"
    DIRECT = "DIRECT"
    ERROR = "ERROR"

class MCPAgent:
    """FastMCPã‚’æ´»ç”¨ã—ãŸMCPã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆ"""
    
    def __init__(self, server_url: str, llm_config: LLMConfig, agent_config: AgentConfig):
        self.server_url = server_url
        self.llm_config = llm_config
        self.agent_config = agent_config
        self.client: Optional[Client] = None
        self.tools: List[Any] = []
        self.llm_client: Optional[OpenAI] = None
        
    async def initialize(self) -> bool:
        """ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã‚’åˆæœŸåŒ–"""
        try:
            # LLMã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã®åˆæœŸåŒ–
            self.llm_client = OpenAI(api_key=self.llm_config.api_key or os.getenv("OPENAI_API_KEY"))
            
            # MCPã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã®åˆæœŸåŒ–
            self.client = Client(self.server_url)
            await self.client.__aenter__()
            
            # ãƒ„ãƒ¼ãƒ«ä¸€è¦§ã®å–å¾—
            self.tools = await self.client.list_tools()
            
            print(f"âœ… ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆåˆæœŸåŒ–å®Œäº†")
            print(f"ğŸ“¡ ã‚µãƒ¼ãƒãƒ¼: {self.server_url}")
            print(f"ğŸ¤– LLM: {self.llm_config.model}")
            print(f"ğŸ”§ åˆ©ç”¨å¯èƒ½ãƒ„ãƒ¼ãƒ«: {[tool.name for tool in self.tools]}")
            
            return True
            
        except Exception as e:
            print(f"âŒ ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆåˆæœŸåŒ–ã‚¨ãƒ©ãƒ¼: {e}")
            return False
    
    async def cleanup(self):
        """ãƒªã‚½ãƒ¼ã‚¹ã®ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—"""
        if self.client:
            try:
                await self.client.__aexit__(None, None, None)
            except Exception as e:
                print(f"âš ï¸ ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—ã‚¨ãƒ©ãƒ¼: {e}")
    
    def _create_tools_schema(self) -> str:
        """åˆ©ç”¨å¯èƒ½ãªãƒ„ãƒ¼ãƒ«ã®ã‚¹ã‚­ãƒ¼ãƒæƒ…å ±ã‚’ç”Ÿæˆ"""
        tools_info = []
        for tool in self.tools:
            schema = getattr(tool, 'inputSchema', {})
            properties = schema.get('properties', {}) if schema else {}
            required = schema.get('required', []) if schema else {}
            
            tool_info = f"- {tool.name}: {tool.description}"
            if properties:
                param_details = []
                for param_name, param_info in properties.items():
                    param_type = param_info.get('type', 'string')
                    is_required = param_name in required
                    param_details.append(f"{param_name}({param_type}{'å¿…é ˆ' if is_required else 'ä»»æ„'})")
                tool_info += f" [ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿: {', '.join(param_details)}]"
            tools_info.append(tool_info)
        
        return "\n".join(tools_info)
    
    async def _ask_llm_for_decision(self, user_input: str) -> Tuple[ToolDecision, str, Dict[str, Any]]:
        """LLMã«ãƒ„ãƒ¼ãƒ«é¸æŠã¨ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ç”Ÿæˆã‚’ä¾é ¼"""
        try:
            tools_schema = self._create_tools_schema()
            
            system_prompt = f"""ã‚ãªãŸã¯ãƒ„ãƒ¼ãƒ«é¸æŠã®å°‚é–€å®¶ã§ã™ã€‚ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®å…¥åŠ›ã«åŸºã¥ã„ã¦ã€æœ€é©ãªãƒ„ãƒ¼ãƒ«ã‚’é¸æŠã—ã€å¿…è¦ãªãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’ç”Ÿæˆã—ã¦ãã ã•ã„ã€‚

åˆ©ç”¨å¯èƒ½ãªãƒ„ãƒ¼ãƒ«:
{tools_schema}

å›ç­”å½¢å¼:
- ãƒ„ãƒ¼ãƒ«ãŒå¿…è¦ãªå ´åˆ: "TOOL: ãƒ„ãƒ¼ãƒ«å" ã®å¾Œã«ã€JSONå½¢å¼ã§ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’å‡ºåŠ›
- ãƒ„ãƒ¼ãƒ«ãŒä¸è¦ãªå ´åˆ: "DIRECT: å…·ä½“çš„ãªå›ç­”å†…å®¹" ã¨å‡ºåŠ›

é‡è¦ãªæŒ‡ç¤º:
1. ãƒ„ãƒ¼ãƒ«å®Ÿè¡Œçµæœã‚’å—ã‘å–ã£ãŸå ´åˆã¯ã€ãã®çµæœã‚’åŸºã«æœ€çµ‚å›ç­”ã‚’ç”Ÿæˆã—ã¦ãã ã•ã„
2. DIRECTå›ç­”ã§ã¯ã€å¿…ãšå…·ä½“çš„ã§æœ‰ç”¨ãªå†…å®¹ã‚’æä¾›ã—ã¦ãã ã•ã„
3. ãƒ„ãƒ¼ãƒ«çµæœã®å ´åˆã¯ã€çµæœã‚’è‡ªç„¶ãªæ—¥æœ¬èªã§èª¬æ˜ã—ã¦ãã ã•ã„

ä¾‹:
- åˆå›å…¥åŠ›: "5 ã¨ 3 ã‚’æ›ã‘ã¦" â†’ å‡ºåŠ›: "TOOL: multiply\n{{"a": 5, "b": 3}}"
- åˆå›å…¥åŠ›: "æ±äº¬ã®å¤©æ°—ã¯ï¼Ÿ" â†’ å‡ºåŠ›: "TOOL: get_weather\n{{"city": "æ±äº¬"}}"
- åˆå›å…¥åŠ›: "ã“ã‚“ã«ã¡ã¯" â†’ å‡ºåŠ›: "DIRECT: ã“ã‚“ã«ã¡ã¯ï¼ä½•ã‹ãŠæ‰‹ä¼ã„ã§ãã‚‹ã“ã¨ã¯ã‚ã‚Šã¾ã™ã‹ï¼Ÿ"
- ãƒ„ãƒ¼ãƒ«çµæœå¾Œ: "çµæœ: get_weather(city=åå¤å±‹) = é›¨ã€æ°—æ¸©25åº¦ã€æ¹¿åº¦80%" â†’ å‡ºåŠ›: "DIRECT: åå¤å±‹ã®å¤©æ°—ã¯é›¨ã§ã€æ°—æ¸©ã¯25åº¦ã€æ¹¿åº¦ã¯80%ã§ã™ã€‚"
- ãƒ„ãƒ¼ãƒ«çµæœå¾Œ: "çµæœ: multiply(a=3, b=6) = 18" â†’ å‡ºåŠ›: "DIRECT: 3ã¨6ã‚’æ›ã‘ã‚‹ã¨18ã«ãªã‚Šã¾ã™ã€‚"

é‡è¦: ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã¯å¿…ãšJSONå½¢å¼ã§å‡ºåŠ›ã—ã€å¿…é ˆãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã¯å¿…ãšå«ã‚ã¦ãã ã•ã„ã€‚"""

            response = self.llm_client.chat.completions.create(
                model=self.llm_config.model,
                messages=[
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": user_input}
                ],
                max_tokens=self.llm_config.max_tokens,
                temperature=self.llm_config.temperature
            )
            
            response_text = response.choices[0].message.content.strip()
            
            # ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã‚’è§£æ
            if response_text.startswith("TOOL:"):
                lines = response_text.split('\n')
                tool_name = lines[0].replace("TOOL:", "").strip()
                
                if len(lines) > 1:
                    try:
                        parameters = json.loads(lines[1])
                        return ToolDecision.TOOL, tool_name, parameters
                    except json.JSONDecodeError:
                        return ToolDecision.ERROR, "JSONãƒ‘ãƒ¼ã‚¹ã‚¨ãƒ©ãƒ¼", {}
                else:
                    return ToolDecision.ERROR, "ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãŒä¸è¶³", {}
                    
            elif response_text.startswith("DIRECT:"):
                direct_answer = response_text.replace("DIRECT:", "").strip()
                return ToolDecision.DIRECT, direct_answer, {}
            else:
                return ToolDecision.ERROR, "ç„¡åŠ¹ãªãƒ¬ã‚¹ãƒãƒ³ã‚¹å½¢å¼", {}
                
        except Exception as e:
            return ToolDecision.ERROR, f"LLMå‘¼ã³å‡ºã—ã‚¨ãƒ©ãƒ¼: {e}", {}
    
    async def _execute_tool(self, tool_name: str, parameters: Dict[str, Any]) -> Tuple[bool, str, Any]:
        """ãƒ„ãƒ¼ãƒ«ã‚’å®Ÿè¡Œ"""
        try:
            # ãƒ„ãƒ¼ãƒ«ã®å­˜åœ¨ç¢ºèª
            selected_tool = None
            for tool in self.tools:
                if tool.name == tool_name:
                    selected_tool = tool
                    break
            
            if not selected_tool:
                return False, f"ãƒ„ãƒ¼ãƒ« '{tool_name}' ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“", None
            
            # ãƒ„ãƒ¼ãƒ«å®Ÿè¡Œ
            result = await self.client.call_tool(tool_name, parameters)
            
            # çµæœã®å‡¦ç†
            if hasattr(result, 'data'):
                actual_result = result.data
            else:
                actual_result = result
            
            return True, "æˆåŠŸ", actual_result
            
        except Exception as e:
            return False, f"ãƒ„ãƒ¼ãƒ«å®Ÿè¡Œã‚¨ãƒ©ãƒ¼: {e}", None
    
    def _format_tool_result(self, tool_name: str, parameters: Dict[str, Any], result: Any) -> str:
        """ãƒ„ãƒ¼ãƒ«çµæœã®è¡¨ç¤ºå½¢å¼ã‚’ç”Ÿæˆ"""
        param_str = ", ".join([f"{k}={v}" for k, v in parameters.items()])
        return f"çµæœ: {tool_name}({param_str}) = {result}"
    
    async def process_query(self, user_input: str) -> str:
        """ã‚¯ã‚¨ãƒªã‚’å‡¦ç†ï¼ˆMCP Loopå®Ÿè£…ï¼‰"""
        if self.agent_config.debug_mode:
            print(f"\nğŸ”„ MCP Loopé–‹å§‹: {user_input}")
            print("=" * 50)
        
        observation = user_input
        max_iterations = self.agent_config.max_iterations
        iteration = 0
        
        while iteration < max_iterations:
            iteration += 1
            
            if self.agent_config.debug_mode:
                print(f"\n--- Iteration {iteration} ---")
                print(f"Observation: {observation}")
            
            # --- Thought (LLMä½¿ç”¨) ---
            if self.agent_config.debug_mode:
                print("\n--- Thought (LLM) ---")
            
            decision, tool_or_answer, parameters = await self._ask_llm_for_decision(observation)
            
            if self.agent_config.debug_mode:
                print(f"Decision: {decision.value}")
                print(f"Tool/Answer: {tool_or_answer}")
                print(f"Parameters: {parameters}")
            
            # --- Action ---
            if self.agent_config.debug_mode:
                print("\n--- Action ---")
            
            if decision == ToolDecision.TOOL:
                # ãƒ„ãƒ¼ãƒ«å®Ÿè¡Œ
                success, message, result = await self._execute_tool(tool_or_answer, parameters)
                
                if success:
                    action_output = self._format_tool_result(tool_or_answer, parameters, result)
                    if self.agent_config.debug_mode:
                        print(f"Action Result: {action_output}")
                else:
                    action_output = f"ã‚¨ãƒ©ãƒ¼: {message}"
                    if self.agent_config.debug_mode:
                        print(f"Action Error: {action_output}")
                
                # æ¬¡ã®åå¾©ã®ãŸã‚ã«è¦³å¯Ÿã‚’æ›´æ–°
                observation = action_output
                
            elif decision == ToolDecision.DIRECT:
                # ç›´æ¥å›ç­”
                action_output = f"å›ç­”: {tool_or_answer}"
                if self.agent_config.debug_mode:
                    print(f"Action Result: {action_output}")
                
                # æœ€çµ‚å›ç­”ã¨ã—ã¦è¿”ã™
                return action_output
                
            else:  # ToolDecision.ERROR
                # ã‚¨ãƒ©ãƒ¼å‡¦ç†
                if self.agent_config.fallback_to_direct:
                    action_output = f"ã‚¨ãƒ©ãƒ¼: {tool_or_answer}ã€‚ç›´æ¥å›ç­”ã‚’è©¦è¡Œã—ã¾ã™ã€‚"
                    if self.agent_config.debug_mode:
                        print(f"Action Error: {action_output}")
                    
                    # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯å‡¦ç†
                    try:
                        fallback_response = self.llm_client.chat.completions.create(
                            model=self.llm_config.model,
                            messages=[
                                {"role": "system", "content": "ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®è³ªå•ã«ç›´æ¥å›ç­”ã—ã¦ãã ã•ã„ã€‚"},
                                {"role": "user", "content": user_input}
                            ],
                            max_tokens=self.llm_config.max_tokens,
                            temperature=self.llm_config.temperature
                        )
                        fallback_answer = fallback_response.choices[0].message.content.strip()
                        return f"å›ç­”: {fallback_answer}"
                    except Exception as e:
                        return f"ç”³ã—è¨³ã‚ã‚Šã¾ã›ã‚“ãŒã€é©åˆ‡ãªå›ç­”ã‚’ç”Ÿæˆã§ãã¾ã›ã‚“ã§ã—ãŸã€‚ã‚¨ãƒ©ãƒ¼: {e}"
                else:
                    return f"ã‚¨ãƒ©ãƒ¼: {tool_or_answer}"
        
        # æœ€å¤§åå¾©å›æ•°ã«é”ã—ãŸå ´åˆ
        if self.agent_config.debug_mode:
            print(f"\nâš ï¸ æœ€å¤§åå¾©å›æ•°({max_iterations})ã«é”ã—ã¾ã—ãŸ")
        
        return "ç”³ã—è¨³ã‚ã‚Šã¾ã›ã‚“ãŒã€å‡¦ç†ãŒå®Œäº†ã—ã¾ã›ã‚“ã§ã—ãŸã€‚æ™‚é–“ã‚’ç½®ã„ã¦å†åº¦ãŠè©¦ã—ãã ã•ã„ã€‚"

async def main():
    """ãƒ¡ã‚¤ãƒ³é–¢æ•°"""
    print("FastMCP Agent - ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆæŠ½è±¡åŒ–ç‰ˆ")
    print("=" * 50)
    
    # è¨­å®šã®åˆæœŸåŒ–
    llm_config = LLMConfig(
        model="gpt-4o-mini",
        temperature=0.1,
        max_tokens=500,
        api_key=os.getenv("OPENAI_API_KEY")
    )
    
    agent_config = AgentConfig(
        max_iterations=5,
        debug_mode=True,
        fallback_to_direct=True,
        timeout=30
    )
    
    # ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã®åˆæœŸåŒ–
    agent = MCPAgent("http://localhost:8001/mcp", llm_config, agent_config)
    
    try:
        # ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆåˆæœŸåŒ–
        if not await agent.initialize():
            print("âŒ ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã®åˆæœŸåŒ–ã«å¤±æ•—ã—ã¾ã—ãŸ")
            return
        
        print("\nâœ… ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆæº–å‚™å®Œäº†ï¼")
        print("çµ‚äº†ã™ã‚‹ã«ã¯ 'quit' ã¾ãŸã¯ 'exit' ã¨å…¥åŠ›ã—ã¦ãã ã•ã„\n")
        
        # ãƒ¡ã‚¤ãƒ³ãƒ«ãƒ¼ãƒ—
        while True:
            try:
                user_input = input("\nUser query: ").strip()
                
                if user_input.lower() in ['quit', 'exit', 'çµ‚äº†']:
                    print("ğŸ‘‹ çµ‚äº†ã—ã¾ã™ã€‚")
                    break
                
                if not user_input:
                    continue
                
                # ã‚¯ã‚¨ãƒªå‡¦ç†
                result = await agent.process_query(user_input)
                print(f"\n{result}")
                
            except KeyboardInterrupt:
                print("\n\nğŸ‘‹ çµ‚äº†ã—ã¾ã™ã€‚")
                break
            except Exception as e:
                print(f"âŒ ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
    
    finally:
        # ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—
        await agent.cleanup()

if __name__ == "__main__":
    asyncio.run(main())
